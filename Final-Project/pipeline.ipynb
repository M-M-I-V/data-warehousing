{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e75c3070-fae5-4815-bee4-19a85861ad20",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "We import all the libraries needed for data cleaning and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e44a6f4c-ea1e-4258-a5d9-00994b0fd63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The primary library for data manipulation and analysis, used for creating and working with DataFrames.\n",
    "import pandas as pd\n",
    "\n",
    "# The fundamental library for numerical computing in Python, often used for array operations and mathematical functions.\n",
    "import numpy as np\n",
    "\n",
    "# A scikit-learn class used for handling missing values (e.g., filling with mean, median, or constant).\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b00a4d-963e-4778-af82-6bf1dd55b180",
   "metadata": {},
   "source": [
    "## Cleaning Diagnoses Dataset\n",
    "Steps performed:\n",
    "1. Load dataset\n",
    "2. Remove duplicates\n",
    "3. Fill missing values for diagnosis code & description\n",
    "4. Remove invalid codes (e.g., \"XXX\")\n",
    "5. Standardize text formatting\n",
    "6. Enforce one-to-one code consistency per diagnosis\n",
    "7. Convert data types\n",
    "8. Export cleaned file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "192291e2-3947-43e3-974a-3f161130df13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnoses Cleaned Dataset:\n",
      "    diagnosis_id diagnosis_code   description\n",
      "0             1           D004      Covid-19\n",
      "1             2           D003  Hypertension\n",
      "2             3           D005           Flu\n",
      "3             4           D004      Covid-19\n",
      "4             5           D004      Covid-19\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "# Primary library for data manipulation and analysis (DataFrames).\n",
    "import pandas as pd\n",
    "# Used for numerical operations, particularly for 'np.nan' (Not a Number).\n",
    "import numpy as np\n",
    "# A scikit-learn tool to handle missing data.\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load Data\n",
    "# Load the raw dataset from a CSV file into a pandas DataFrame.\n",
    "df = pd.read_csv(\"diagnoses.csv\")\n",
    "\n",
    "# Initial Cleaning\n",
    "# Remove any rows that are exact duplicates to ensure data integrity.\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Impute Missing Values\n",
    "# We assume that missing codes/descriptions are rare and can be filled\n",
    "# with the most common (mode) value from their respective columns.\n",
    "\n",
    "# Impute missing 'diagnosis_code'.\n",
    "# Note: fit_transform expects a 2D array, so we pass [['diagnosis_code']].\n",
    "# .ravel() is then used to \"flatten\" the 2D output back into a 1D series\n",
    "# for assignment to the DataFrame column.\n",
    "df[\"diagnosis_code\"] = SimpleImputer(strategy=\"most_frequent\") \\\n",
    "    .fit_transform(df[[\"diagnosis_code\"]]).ravel()\n",
    "\n",
    "# Impute missing 'description' using the same \"most_frequent\" strategy.\n",
    "df[\"description\"] = SimpleImputer(strategy=\"most_frequent\") \\\n",
    "    .fit_transform(df[[\"description\"]]).ravel()\n",
    "\n",
    "# Filter Invalid Data\n",
    "# Remove rows where 'diagnosis_code' is \"XXX\", which is treated as a\n",
    "# placeholder or invalid entry according to business rules.\n",
    "df = df[df[\"diagnosis_code\"] != \"XXX\"]\n",
    "\n",
    "# Standardize Text Descriptions\n",
    "# Clean and standardize the 'description' text to ensure consistency.\n",
    "# .str.strip(): Remove leading/trailing whitespace.\n",
    "# .str.title(): Capitalize the first letter of each word (e.g., \"heart attack\" -> \"Heart Attack\").\n",
    "df[\"description\"] = df[\"description\"].str.strip().str.title()\n",
    "# Correct known abbreviations for uniformity (e.g., \"Unk\" -> \"Unknown\").\n",
    "df[\"description\"] = df[\"description\"].str.replace(\"Unk\", \"Unknown\")\n",
    "\n",
    "# Create a Consistent Description-to-Code Mapping ---\n",
    "# GOAL: Ensure that every unique description maps to one, and only one,\n",
    "# unique diagnosis code. This enforces a one-to-one relationship.\n",
    "\n",
    "# Find the \"preferred\" code (mode) for each description.\n",
    "# Group by the cleaned description and find the most frequent (mode)\n",
    "# diagnosis code associated with it.\n",
    "mode_mapping = df.groupby(\"description\")[\"diagnosis_code\"].agg(\n",
    "    # Use a lambda to safely get the mode. If the mode is empty (e.g., no data),\n",
    "    # return np.nan to avoid an error.\n",
    "    lambda x: x.mode()[0] if not x.mode().empty else np.nan\n",
    ").to_dict()\n",
    "\n",
    "# Define the pool of descriptions and codes to work with.\n",
    "# We'll handle 'Unknown' separately at the end.\n",
    "primary_descriptions = [desc for desc in mode_mapping if desc != 'Unknown']\n",
    "\n",
    "# Get a set of all unique codes present in the *original* data.\n",
    "# This is our pool of available codes.\n",
    "all_codes = set(df['diagnosis_code'].unique())\n",
    "# Keep track of codes we've already assigned in our new map.\n",
    "used_codes = set()\n",
    "# This dictionary will hold our final, clean, 1:1 mapping.\n",
    "consistent_map_final = {}\n",
    "\n",
    "# Iterate and resolve conflicts to build the final map.\n",
    "for desc in primary_descriptions:\n",
    "    suggested_code = mode_mapping[desc]\n",
    "\n",
    "    # HAPPY PATH\n",
    "    # If the preferred code (mode) hasn't been assigned yet, assign it.\n",
    "    if suggested_code not in used_codes:\n",
    "        consistent_map_final[desc] = suggested_code\n",
    "        used_codes.add(suggested_code)\n",
    "    \n",
    "    # ONFLICT PATH\n",
    "    # The preferred code is already assigned to another description.\n",
    "    # We must find a new, unique code for this description.\n",
    "    else:\n",
    "        # Find all codes from the original dataset that are not yet used.\n",
    "        # Sort them (key=str) to ensure deterministic assignment (e.g., \"C100\" before \"C99\").\n",
    "        available_codes = sorted(all_codes - used_codes, key=str)\n",
    "\n",
    "        if available_codes:\n",
    "            # If there's an unused code from the original pool, assign the first one.\n",
    "            new_code = available_codes[0]\n",
    "        else:\n",
    "            # FALLBACK\n",
    "            # No unused original codes are left. We must generate a new, synthetic code.\n",
    "            # Start from \"C999\" and increment until we find an unused code.\n",
    "            new_code = \"C999\"\n",
    "            while new_code in used_codes:\n",
    "                # Increment the numeric part (e.g., \"C999\" -> \"C1000\")\n",
    "                new_code = f\"C{int(new_code[1:]) + 1}\"\n",
    "\n",
    "        # Assign the new code (either from the pool or synthetic) to the map.\n",
    "        consistent_map_final[desc] = new_code\n",
    "        used_codes.add(new_code)\n",
    "\n",
    "# Handle the 'Unknown' description.\n",
    "# Add 'Unknown' back into the map with its original preferred code.\n",
    "if 'Unknown' in mode_mapping:\n",
    "    consistent_map_final['Unknown'] = mode_mapping['Unknown']\n",
    "    # Note: We add this last and don't check for conflicts, assuming\n",
    "    # its preferred code is the one we want it to have.\n",
    "\n",
    "# Apply the Consistent Mapping\n",
    "# Overwrite the 'diagnosis_code' column by mapping each 'description'\n",
    "# to its new, consistent code from our final map.\n",
    "df[\"diagnosis_code\"] = df[\"description\"].map(consistent_map_final)\n",
    "\n",
    "# Enforce Final Data Types\n",
    "# Convert columns to their proper data types for memory efficiency\n",
    "# and to prevent errors in downstream systems (e.g., databases).\n",
    "df = df.astype({\n",
    "    \"diagnosis_id\": \"int\",\n",
    "    \"diagnosis_code\": \"str\",\n",
    "    \"description\": \"str\"\n",
    "})\n",
    "\n",
    "# Output Results\n",
    "# Print the head of the cleaned DataFrame to the console for verification.\n",
    "print(\"Diagnoses Cleaned Dataset:\\n\", df.head())\n",
    "\n",
    "# Save the fully cleaned and standardized DataFrame to a new CSV file.\n",
    "# index=False prevents pandas from writing the DataFrame index as a column.\n",
    "df.to_csv(\"diagnoses_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b402750c-fb26-4431-bb68-2874ec36e8f8",
   "metadata": {},
   "source": [
    "## Cleaning Patients Dataset\n",
    "Steps performed:\n",
    "1. Load dataset\n",
    "2. Fill missing names\n",
    "3. Fix age errors and impute missing values\n",
    "4. Normalize gender values\n",
    "5. Remove duplicate patient records\n",
    "6. Export cleaned file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd8def15-3897-4b4a-b660-b78415d5a67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients Cleaned Dataset:\n",
      "    patient_id first_name last_name  gender  age\n",
      "0           1      Alice    Wilson  Female   52\n",
      "1           2    Charlie     Smith    Male   93\n",
      "2           3       Jane   Johnson  Female   15\n",
      "3           4      Ethan     Smith  Female   72\n",
      "4           5       Jane    Miller  Female   61\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "# Load the raw patient dataset from its CSV source.\n",
    "df = pd.read_csv(\"patients.csv\")\n",
    "\n",
    "# Clean Text Fields (Names)\n",
    "# Handle missing first names: Fill with a placeholder 'Unknown' for consistency.\n",
    "# Downstream systems may require a non-null value.\n",
    "df['first_name'] = df['first_name'].fillna(\"Unknown\")\n",
    "\n",
    "# Clean last names:\n",
    "# .str.rstrip(\"#\"): Remove any trailing '#' characters, likely from a data entry artifact.\n",
    "# .str.title(): Standardize to 'Title Case' (e.g., \"smith\" -> \"Smith\").\n",
    "df['last_name'] = df['last_name'].str.rstrip(\"#\").str.title()\n",
    "\n",
    "# --- 3. Validate and Impute Numeric Fields (Age) ---\n",
    "# Step 3a: Nullify invalid age entries.\n",
    "# Apply business rule: Ages must be realistic. Set any age\n",
    "# less than 0 or greater than 120 to 'None' (which becomes np.nan)\n",
    "# so they can be imputed in the next step.\n",
    "df.loc[(df['age'] < 0) | (df['age'] > 120), 'age'] = None\n",
    "\n",
    "# Impute missing ages.\n",
    "# Use 'mean' imputation strategy. This fills all NaN/None values in the 'age'\n",
    "# column with the average age from the entire dataset.\n",
    "# Note: [['age']] is used to pass a 2D array (DataFrame) to fit_transform.\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[['age']] = imputer.fit_transform(df[['age']])\n",
    "\n",
    "# Convert age to integer.\n",
    "# After imputation (which yields floats), cast the column to 'int'\n",
    "# for a cleaner, more appropriate data type.\n",
    "df['age'] = df['age'].astype(int)\n",
    "\n",
    "# Normalize Categorical Fields (Gender)\n",
    "# Standardize the 'gender' column to have consistent values.\n",
    "# .str.strip(): Remove leading/trailing whitespace.\n",
    "# .str.upper(): Convert all to uppercase to handle 'm', 'f', 'M', 'F', etc.\n",
    "# .replace(): Map the standardized abbreviations to the full, desired terms.\n",
    "df['gender'] = df['gender'].str.strip().str.upper().replace({\n",
    "    'M': 'Male',\n",
    "    'F': 'Female'\n",
    "})\n",
    "\n",
    "# Deduplicate Records\n",
    "# Ensure each patient is represented only once by dropping duplicate rows\n",
    "# based on the 'patient_id' column, which is our unique business key.\n",
    "df = df.drop_duplicates(subset=['patient_id'])\n",
    "\n",
    "# Output Results\n",
    "# Print the head of the cleaned DataFrame to the console for verification.\n",
    "print(\"Patients Cleaned Dataset:\\n\", df.head())\n",
    "\n",
    "# Save the fully cleaned DataFrame to a new CSV file.\n",
    "# index=False prevents pandas from writing the DataFrame row index as a new column.\n",
    "df.to_csv(\"patients_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4e92f2-6fa5-499e-bbc8-ae878aaa755f",
   "metadata": {},
   "source": [
    "## Cleaning Treatments Dataset\n",
    "Steps performed:\n",
    "1. Load dataset and remove duplicates\n",
    "2. Fix missing doctor names and costs\n",
    "3. Replace invalid entries\n",
    "4. Clean text formatting\n",
    "5. Convert dates properly\n",
    "6. Handle negative/extreme treatment costs\n",
    "7. Convert data types\n",
    "8. Export cleaned file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dec30450-11ec-4621-8a86-05a1912ac686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treatments Cleaned Dataset:\n",
      "    admission_id  patient_id doctor_name          department  diagnosis_id  \\\n",
      "0             1          50   Dr. Evans            Oncology             1   \n",
      "1             2          61   Dr. Evans           Neurology            78   \n",
      "2             3          51   Dr. Adams           Neurology           115   \n",
      "3             4          19   Dr. Adams  Unknown Department            47   \n",
      "4             5          21   Dr. Evans          Pediatrics           114   \n",
      "\n",
      "  admission_date  treatment_cost  \n",
      "0     2023-05-08         49854.0  \n",
      "1     2023-12-22         21607.0  \n",
      "2     2023-10-06         38599.0  \n",
      "3     2023-02-24         29529.0  \n",
      "4     2023-10-20         36954.0  \n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "# Import pandas for DataFrame manipulation.\n",
    "import pandas as pd\n",
    "# Import numpy for numerical operations, specifically np.nan.\n",
    "import numpy as np\n",
    "# Import SimpleImputer for handling missing data.\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load Data\n",
    "# Load the raw treatments dataset from a CSV file.\n",
    "df = pd.read_csv(\"treatments.csv\")\n",
    "\n",
    "# Deduplication\n",
    "# Remove any rows that are exact duplicates across all columns.\n",
    "# .copy() is used to avoid a SettingWithCopyWarning in later operations.\n",
    "df = df.drop_duplicates().copy()\n",
    "\n",
    "# Initial Imputation (Missing Values)\n",
    "# Fill missing 'doctor_name' with the most frequent (mode) name in the dataset.\n",
    "df[\"doctor_name\"] = SimpleImputer(strategy=\"most_frequent\") \\\n",
    "    .fit_transform(df[[\"doctor_name\"]]).ravel()\n",
    "\n",
    "# Fill missing 'treatment_cost' with the median cost. Median is chosen\n",
    "# as it is robust to outliers, which are common in cost data.\n",
    "df[\"treatment_cost\"] = SimpleImputer(strategy=\"median\") \\\n",
    "    .fit_transform(df[[\"treatment_cost\"]]).ravel()\n",
    "\n",
    "# Fix and Standardize Invalid Text Entries\n",
    "# Replace placeholder '???' entries in 'doctor_name' with a standard 'Unknown' value.\n",
    "# (na=False ensures we don't try to run .str.contains on NaN values).\n",
    "df.loc[df[\"doctor_name\"].str.contains(\"\\?\\?\\?\", na=False), \"doctor_name\"] = \"Unknown Doctor\"\n",
    "\n",
    "# Standardize department names: correct known abbreviations.\n",
    "df.loc[df[\"department\"] == \"UnknownDept\", \"department\"] = \"Unknown Department\"\n",
    "\n",
    "# --- 5. Standardize Text Formatting ---\n",
    "# Apply consistent text formatting to string columns.\n",
    "# .str.strip(): Remove leading/trailing whitespace.\n",
    "# .str.title(): Convert to Title Case (e.g., \"john doe\" -> \"John Doe\").\n",
    "df[\"doctor_name\"] = df[\"doctor_name\"].str.strip().str.title()\n",
    "df[\"department\"] = df[\"department\"].str.strip().str.title()\n",
    "\n",
    "# Convert Date Values\n",
    "# Convert the 'admission_date' column to datetime objects.\n",
    "# errors='coerce': If any date cannot be parsed, it will be set to NaT (Not a Time).\n",
    "df[\"admission_date\"] = pd.to_datetime(df[\"admission_date\"], errors=\"coerce\")\n",
    "\n",
    "# Validate and Clean Numeric Fields (Cost)\n",
    "# Apply business rules to the 'treatment_cost' column.\n",
    "\n",
    "# Nullify invalid negative costs.\n",
    "# Set any costs less than 0 to NaN, as negative costs are invalid.\n",
    "df.loc[df[\"treatment_cost\"] < 0, \"treatment_cost\"] = np.nan\n",
    "\n",
    "# Re-impute.\n",
    "# Fill the NaNs we just created (from negative values) using the median strategy.\n",
    "# This ensures we don't have invalid negative numbers skewing our data.\n",
    "df[\"treatment_cost\"] = SimpleImputer(strategy=\"median\").fit_transform(df[[\"treatment_cost\"]])\n",
    "\n",
    "# Cap outliers (Top-coding).\n",
    "# Set any costs greater than 200,000 (a business-defined ceiling)\n",
    "# to 200,000. This prevents extreme outliers from skewing analysis.\n",
    "df.loc[df[\"treatment_cost\"] > 200000, \"treatment_cost\"] = 200000\n",
    "\n",
    "# Enforce Final Data Types\n",
    "# Convert columns to their final, appropriate data types for memory\n",
    "# efficiency and compatibility with downstream systems.\n",
    "# Note: 'treatment_cost' remains float, 'admission_date' remains datetime.\n",
    "df = df.astype({\n",
    "    \"admission_id\": \"int\",\n",
    "    \"patient_id\": \"int\",\n",
    "    \"diagnosis_id\": \"int\",\n",
    "    \"department\": \"str\",\n",
    "    \"doctor_name\": \"str\"\n",
    "})\n",
    "\n",
    "# Output Results\n",
    "# Print the head of the cleaned DataFrame to the console for verification.\n",
    "print(\"Treatments Cleaned Dataset:\\n\", df.head())\n",
    "\n",
    "# Save the cleaned data to a new CSV file without the pandas index.\n",
    "df.to_csv(\"treatments_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac48f3f0-debe-4370-9632-b73ae6dcadd9",
   "metadata": {},
   "source": [
    "## SQL Script Execution for ETL\n",
    "\n",
    "This step runs the `script.sql` file which contains all SQL statements for:\n",
    "1. Creating OLTP tables  \n",
    "2.  Loading CSV datasets  \n",
    "3.  Creating Data Warehouse dimension & fact tables  \n",
    "4.  Transforming and inserting data into the DW\n",
    "\n",
    "Execution is done using a database transaction:\n",
    "- If all commands succeed → Commit to MySQL\n",
    "- If any command fails → Rollback all changes\n",
    "\n",
    "A log file named `etl_sql_execution.log` is generated to store execution details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8bb9ecb-a9a0-4224-9f85-c0cbcd9031da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL script executed successfully!\n"
     ]
    }
   ],
   "source": [
    "# The specific Python driver used to interact with MySQL databases.\n",
    "import mysql.connector\n",
    "# Standard library for writing logs, essential for tracking ETL processes.\n",
    "import logging\n",
    "# Standard library for interacting with the OS, used here to check file existence.\n",
    "import os\n",
    "\n",
    "# Logger Setup\n",
    "# Configure a file-based logger. In data engineering, stdout (print) is temporary,\n",
    "# but a log file provides a persistent record for debugging failed runs.\n",
    "logging.basicConfig(\n",
    "    filename=\"etl_sql_execution.log\",  # Log file name.\n",
    "    level=logging.INFO,  # Set the logging level to INFO (captures info, warnings, errors).\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"  # Define a clear log format.\n",
    ")\n",
    "\n",
    "# Configuration & Pre-flight Check ---\n",
    "# Define the SQL script to be executed.\n",
    "SQL_FILE = \"script.sql\"\n",
    "\n",
    "# Pre-check: Fail fast if the SQL script is missing.\n",
    "# This avoids opening a database connection only to fail, saving resources.\n",
    "if not os.path.exists(SQL_FILE):\n",
    "    logging.error(f\"Required {SQL_FILE} not found! Halting execution.\")\n",
    "    raise FileNotFoundError(f\"{SQL_FILE} not found!\")\n",
    "\n",
    "# Database Connection & Transaction\n",
    "# Initialize connection and cursor variables outside the try block.\n",
    "# This ensures they are accessible in the 'finally' block for cleanup,\n",
    "# even if the connection fails.\n",
    "connection = None\n",
    "cursor = None\n",
    "\n",
    "try:\n",
    "    # Connect to MySQL ---\n",
    "    logging.info(\"Attempting to connect to MySQL database 'healthcare_dw'...\")\n",
    "    connection = mysql.connector.connect(\n",
    "        host=\"localhost\",  # DB server address\n",
    "        user=\"root\",  # Username (should be a service account in production)\n",
    "        password=\"\",  # Password (should be from env variables/secrets manager)\n",
    "        database=\"healthcare_dw\",  # The specific data warehouse to use\n",
    "        allow_local_infile=True  # A flag often needed for 'LOAD DATA' commands\n",
    "    )\n",
    "    logging.info(\"Database connection successful.\")\n",
    "\n",
    "    # Create a cursor object, which is the interface for executing queries.\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Load SQL Script\n",
    "    # Read the entire content of the .sql file into a string variable.\n",
    "    with open(SQL_FILE, \"r\") as file:\n",
    "        sql_script = file.read()\n",
    "    logging.info(f\"SQL script '{SQL_FILE}' loaded into memory.\")\n",
    "\n",
    "    # Execute Script Transactionally\n",
    "    # Split the script into individual statements using the semicolon as a delimiter.\n",
    "    # This allows for basic multi-statement execution.\n",
    "    statements = sql_script.split(';')\n",
    "    executed_count = 0\n",
    "\n",
    "    logging.info(\"Starting execution of SQL statements...\")\n",
    "    for statement in statements:\n",
    "        # .strip() removes any leading/trailing whitespace (like newlines).\n",
    "        # This prevents errors from executing \"empty\" statements.\n",
    "        stmt = statement.strip()\n",
    "        if stmt:  # Only execute if the statement is not empty.\n",
    "            cursor.execute(stmt)\n",
    "            executed_count += 1\n",
    "            # Log the first 50 chars for traceability without flooding the log file.\n",
    "            logging.info(f\"Executed statement: {stmt[:50]}...\")\n",
    "\n",
    "    # Commit Transaction\n",
    "    # If all statements executed successfully, commit the changes.\n",
    "    # This makes the changes (INSERTs, UPDATEs, etc.) permanent in the database.\n",
    "    # This is the \"all\" part of an \"all-or-nothing\" transaction.\n",
    "    connection.commit()\n",
    "    logging.info(f\"Script executed successfully. Committed {executed_count} statements.\")\n",
    "    print(\"SQL script executed successfully!\")\n",
    "\n",
    "except mysql.connector.Error as err:\n",
    "    # Error Handling & Rollback\n",
    "    # This block catches any database-specific errors (e.g., syntax, connection issues).\n",
    "    logging.error(f\"A MySQL error occurred: {err}\")\n",
    "    if connection:\n",
    "        # If an error occurred, roll back *all* changes made during this 'try' block.\n",
    "        # This is the \"nothing\" part of an \"all-or-nothing\" transaction,\n",
    "        # ensuring the database is left in a consistent state.\n",
    "        connection.rollback()\n",
    "        logging.warning(\"Transaction rolled back due to error. No changes were committed.\")\n",
    "    print(f\"Execution failed. Rolled back changes: {err}\")\n",
    "\n",
    "finally:\n",
    "    # Cleanup\n",
    "    # This block *always* executes, whether the 'try' succeeded or failed.\n",
    "    # Its purpose is to close connections and free up database resources.\n",
    "    logging.info(\"Closing database connection and cursor...\")\n",
    "    if cursor:\n",
    "        cursor.close()\n",
    "    if connection:\n",
    "        connection.close()\n",
    "    logging.info(\"Database connection closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e86985c-3b9a-4432-9ddf-23b73006d323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
